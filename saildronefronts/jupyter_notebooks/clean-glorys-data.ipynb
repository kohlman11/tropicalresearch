{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64664703-3814-41de-b97e-4ef24247fb48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob\n",
    "\n",
    "# Path to data\n",
    "path = '/Volumes/external/TIW/data/glorys/'\n",
    "\n",
    "# Find files\n",
    "files = glob.glob(f'{path}*.nc')\n",
    "\n",
    "# Separate files by resolution\n",
    "high_res_files = [f for f in files if '0.083deg' in f]\n",
    "low_res_files = [f for f in files if '0.25deg' in f]\n",
    "\n",
    "# Variables we want to keep\n",
    "vars_to_keep = ['thetao', 'so', 'uo', 'vo']\n",
    "\n",
    "# Open high-resolution data lazily and drop unnecessary variables\n",
    "ds_high_res = xr.open_mfdataset(\n",
    "    high_res_files,\n",
    "    combine='by_coords',\n",
    "    chunks={'time': 100},  # Adjust for your machine\n",
    "    parallel=True,\n",
    "    drop_variables=[var for var in xr.open_dataset(high_res_files[0]).data_vars if var not in vars_to_keep]\n",
    ")\n",
    "\n",
    "# Load low-resolution file\n",
    "ds_low_res = xr.open_dataset(low_res_files[0])\n",
    "\n",
    "print('files are opened')\n",
    "\n",
    "# Rename low-resolution variables to match high-resolution\n",
    "rename_dict = {\n",
    "    'thetao_glor': 'thetao',\n",
    "    'so_glor': 'so',\n",
    "    'uo_glor': 'uo',\n",
    "    'vo_glor': 'vo'\n",
    "}\n",
    "ds_low_res = ds_low_res.rename(rename_dict)\n",
    "\n",
    "# Select only needed variables after renaming\n",
    "ds_low_res = ds_low_res[vars_to_keep]\n",
    "\n",
    "# Align low-res (0.25°) to high-res (0.083°) grid if necessary\n",
    "if ds_low_res.longitude.shape != ds_high_res.longitude.shape or ds_low_res.latitude.shape != ds_high_res.latitude.shape:\n",
    "    ds_low_res = ds_low_res.interp(\n",
    "        longitude=ds_high_res.longitude,\n",
    "        latitude=ds_high_res.latitude,\n",
    "        method='linear'\n",
    "    )\n",
    "\n",
    "\n",
    "ds_high_res_before = ds_high_res.sel(time=slice(None, \"2021-06-29\"))\n",
    "ds_low_res_gap = ds_low_res.sel(time=slice(\"2021-06-30\", \"2022-06-01\"))\n",
    "ds_high_res_after = ds_high_res.sel(time=slice(\"2022-06-02\", None))\n",
    "\n",
    "\n",
    "combined = xr.concat(\n",
    "    [ds_high_res_before, ds_low_res_gap, ds_high_res_after],\n",
    "    dim='time',\n",
    "    #combine_attrs='override'\n",
    ")\n",
    "combined = combined.sortby(\"time\")\n",
    "\n",
    "# Save final combined dataset\n",
    "combined.to_netcdf(f'{path}/glorys_2017_2025_cleaned_0.083deg.nc', engine='netcdf4', compute=True)\n",
    "\n",
    "# issue: not properly concactonating the low resoltuion data to the high resolution data gap..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb0b58-880a-4207-bd11-0d8fabe8800d",
   "metadata": {},
   "source": [
    "attempt to clean the data... maybe just do it aug-feb of each year and save as netcdf chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92bad1b-1247-440e-a0b2-36c4e70da6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt, hilbert\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Paths\n",
    "path = '/Volumes/external/TIW/data/glorys/'\n",
    "output_dir = os.path.join(path, 'winters')\n",
    "phase_dir = '/Users/katiekohlman/Desktop/TIW/netCDF/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(phase_dir, exist_ok=True)\n",
    "\n",
    "# Find files\n",
    "files = glob.glob(f'{path}*.nc')\n",
    "\n",
    "# Separate files by resolution\n",
    "high_res_files = [f for f in files if '0.083deg' in f]\n",
    "low_res_files = [f for f in files if '0.25deg' in f]\n",
    "\n",
    "# Variables to process\n",
    "vars_to_keep = ['thetao', 'so', 'uo', 'vo']\n",
    "\n",
    "# Target chunking (adjust based on your machine)\n",
    "target_chunks = {'time': 10, 'depth': 50, 'latitude': 200, 'longitude': 200}\n",
    "\n",
    "# Open high-res data lazily\n",
    "ds_high_res = xr.open_mfdataset(\n",
    "    high_res_files,\n",
    "    combine='by_coords',\n",
    "    chunks=target_chunks,\n",
    "    parallel=True,\n",
    "    drop_variables=[var for var in xr.open_dataset(high_res_files[0]).data_vars if var not in vars_to_keep]\n",
    ")\n",
    "\n",
    "# Open and rename low-res data\n",
    "ds_low_res = xr.open_dataset(low_res_files[0], chunks=target_chunks)\n",
    "ds_low_res = ds_low_res.rename({\n",
    "    'thetao_glor': 'thetao',\n",
    "    'so_glor': 'so',\n",
    "    'uo_glor': 'uo',\n",
    "    'vo_glor': 'vo'\n",
    "})[vars_to_keep]\n",
    "\n",
    "# Interpolate low-res to high-res grid (one-time process if not already saved)\n",
    "interp_path = f'{path}/low_res_interp_to_high_res_grid.zarr'\n",
    "\n",
    "if not os.path.exists(interp_path):\n",
    "    ds_low_res = ds_low_res.interp(\n",
    "        longitude=ds_high_res.longitude,\n",
    "        latitude=ds_high_res.latitude,\n",
    "        method='linear'\n",
    "    )\n",
    "    ds_low_res.to_zarr(interp_path, mode='w')\n",
    "else:\n",
    "    ds_low_res = xr.open_zarr(interp_path)\n",
    "\n",
    "# Time slices (gap handling)\n",
    "ds_high_res_before = ds_high_res.sel(time=slice(None, \"2021-06-29\"))\n",
    "ds_low_res_gap = ds_low_res.sel(time=slice(\"2021-06-30\", \"2022-06-01\"))\n",
    "ds_high_res_after = ds_high_res.sel(time=slice(\"2022-06-02\", None))\n",
    "\n",
    "print(\"interpolating done\")\n",
    "\n",
    "# Define winter season ranges\n",
    "seasons = [\n",
    "    ('2017-08-01', '2018-02-28'),\n",
    "    ('2018-08-01', '2019-02-28'),\n",
    "    ('2019-08-01', '2020-02-29'),  # Leap year!\n",
    "    ('2020-08-01', '2021-02-28'),\n",
    "    ('2021-08-01', '2022-02-28'),\n",
    "    ('2022-08-01', '2023-02-28'),\n",
    "    ('2023-08-01', '2024-02-29'),\n",
    "    ('2024-08-01', '2025-02-28'),\n",
    "]\n",
    "\n",
    "# Function to compute phase signal for temperature\n",
    "def compute_phase_signal(winter_file, start, end):\n",
    "    ds = xr.open_dataset(winter_file)\n",
    "\n",
    "    # Surface temperature processing\n",
    "    sfc_depth = 0\n",
    "    temp_sfc = ds.thetao.sel(depth=sfc_depth, method='nearest')\n",
    "\n",
    "    # Time mean and anomaly\n",
    "    temp_time_mean_sfc = temp_sfc.mean(dim=\"time\")\n",
    "    temp_anomaly_sfc = temp_sfc - temp_time_mean_sfc\n",
    "\n",
    "    # Detrend\n",
    "    trend_sfc = temp_anomaly_sfc.polyfit(dim=\"time\", deg=1)\n",
    "    trend_fit_sfc = xr.polyval(temp_sfc.time, trend_sfc.polyfit_coefficients)\n",
    "    temp_anomaly_detrended_sfc = temp_anomaly_sfc - trend_fit_sfc\n",
    "\n",
    "    # Bandpass filter\n",
    "    time_step_days = np.mean(np.diff(temp_sfc.time).astype('timedelta64[s]').astype(int)) / (60 * 60 * 24)\n",
    "    nyquist = 0.5 * (1 / time_step_days)\n",
    "    low_cutoff = 1 / 50\n",
    "    high_cutoff = 1 / 15\n",
    "    low, high = low_cutoff / nyquist, high_cutoff / nyquist\n",
    "\n",
    "    b, a = butter(4, [low, high], btype='band')\n",
    "    filtered_temp_sfc = filtfilt(b, a, temp_sfc)\n",
    "\n",
    "    # Create dataset for phase processing\n",
    "    ds_phase = xr.Dataset(\n",
    "        {\n",
    "            \"temp_sfc\": ([\"time\", \"latitude\", \"longitude\"], temp_sfc.data),\n",
    "            \"temp_anomaly_sfc\": ([\"time\", \"latitude\", \"longitude\"], temp_anomaly_sfc.data),\n",
    "            \"temp_butter_sfc\": ([\"time\", \"latitude\", \"longitude\"], filtered_temp_sfc),\n",
    "            \"temp_anomaly_detrended_sfc\": ([\"time\", \"latitude\", \"longitude\"], temp_anomaly_detrended_sfc.data),\n",
    "        },\n",
    "        coords={\n",
    "            \"time\": temp_sfc.time,\n",
    "            \"latitude\": ds.latitude,\n",
    "            \"longitude\": ds.longitude,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Hilbert transform and phase computation\n",
    "    for var_name in ['temp_butter_sfc', 'temp_anomaly_sfc', 'temp_anomaly_detrended_sfc']:\n",
    "        analytic_signal = hilbert(ds_phase[var_name], axis=0)\n",
    "        phase = np.angle(analytic_signal, deg=False)\n",
    "        phase_var_name = var_name.replace('temp_', '').replace('_sfc', '')\n",
    "        ds_phase[f'phase_{phase_var_name}'] = (('time', 'latitude', 'longitude'), phase)\n",
    "\n",
    "    # Save phase file\n",
    "    phase_file = f'{phase_dir}/glorys_thetao_phase_{start[:4]}_{end[:4]}.nc'\n",
    "    ds_phase.to_netcdf(phase_file)\n",
    "    print(f'Saved phase data to: {phase_file}')\n",
    "\n",
    "# Process each variable and season\n",
    "for var in vars_to_keep:\n",
    "    for start, end in seasons:\n",
    "        if start <= \"2021-06-29\":\n",
    "            ds_season = ds_high_res_before[var].sel(time=slice(start, end))\n",
    "        elif start >= \"2022-06-02\":\n",
    "            ds_season = ds_high_res_after[var].sel(time=slice(start, end))\n",
    "        else:\n",
    "            ds_part1 = ds_high_res_before[var].sel(time=slice(start, \"2021-06-29\"))\n",
    "            ds_part2 = ds_low_res_gap[var].sel(time=slice(\"2021-06-30\", \"2022-06-01\"))\n",
    "            ds_part3 = ds_high_res_after[var].sel(time=slice(\"2022-06-02\", end))\n",
    "            ds_season = xr.concat([ds_part1, ds_part2, ds_part3], dim='time').sortby('time')\n",
    "\n",
    "        if len(ds_season.time) == 0:\n",
    "            continue\n",
    "\n",
    "        winter_file = os.path.join(output_dir, f'glorys_{var}_winter_{start[:4]}_{end[:4]}.nc')\n",
    "        ds_season.to_netcdf(winter_file)\n",
    "\n",
    "        # Run phase computation for temperature after saving thetao\n",
    "        if var == 'thetao':\n",
    "            compute_phase_signal(winter_file, start, end)\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f08fd48-a554-45ce-a762-949ff5b2d15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa888124-2102-4134-a316-5c6a621f31e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968720b3-00f8-49e2-b929-1703ecddbc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1adb4e-fddf-4684-a6df-984eed9be709",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_high_res_before.time.values)\n",
    "print(ds_low_res_gap.time.values)\n",
    "print(ds_high_res_after.time.values)\n",
    "\n",
    "print(ds_high_res.latitude.values)\n",
    "print(ds_low_res_gap.latitude.values)\n",
    "\n",
    "print(ds_high_res.longitude.values)\n",
    "print(ds_low_res_gap.longitude.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f6cc88-1df3-4330-a3f3-1fc9f58e9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = xr.combine_by_coords([ds_high_res_before, ds_low_res_gap, ds_high_res_after], combine_attrs='override')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e55e5c8-7f5a-4fa5-8a51-948565169da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds_high_res_before.time,ds_high_res_before.sel(depth = 0, latitude = 2, longitude =-140, method='nearest').thetao)\n",
    "plt.plot(ds_low_res_gap.time,ds_low_res_gap.sel(depth = 0, latitude = 2, longitude =-140, method='nearest').thetao)\n",
    "plt.plot(ds_high_res_after.time,ds_high_res_after.sel(depth = 0, latitude = 2, longitude =-140, method='nearest').thetao)\n",
    "plt.xlim(np.datetime64(\"2021-01-30\"), np.datetime64(\"2023-01-30\"))\n",
    "\n",
    "\n",
    "plt.plot(combined2.time,combined2.sel(depth = 0, latitude = 2, longitude =-140, method='nearest').thetao)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
